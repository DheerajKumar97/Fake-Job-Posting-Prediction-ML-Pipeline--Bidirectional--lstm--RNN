{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this Dataset\n",
    "\n",
    "**[Real or Fake] : Fake Job Description Prediction \n",
    "<br>\n",
    "This dataset contains 18K job descriptions out of which about 800 are fake. The data consists of both textual information and meta-information about the jobs. The dataset can be used to create classification models which can learn the job descriptions which are fraudulent.**\n",
    "\n",
    "\n",
    "**Download Data From**\n",
    "https://www.kaggle.com/shivamb/real-or-fake-fake-jobposting-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from sklearn import preprocessing\n",
    "from tensorflow import keras\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 =\"fake_job_postings.csv\"\n",
    "class DataFrame_Loader():\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        pass\n",
    "        \n",
    "    def load_data_files(self,path1):\n",
    "        dftrain = pd.read_csv(path1)\n",
    "        return dftrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrame_Preprocessor():\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def Preprocessor(self,data):\n",
    "        \n",
    "        data = data[[\"title\", \"company_profile\", \"description\", \"requirements\", \"benefits\",\"fraudulent\"]]\n",
    "        data = data.fillna(' ')\n",
    "        xdf = data[data.columns[0:-1]].apply(lambda x: ','.join(x.dropna().astype(str)),axis=1)\n",
    "        target = data['fraudulent']\n",
    "        return xdf, target\n",
    "    \n",
    "    def clean_text(self,text):\n",
    "        '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "        and remove words containing numbers.'''\n",
    "        text = text.lower()\n",
    "        text = re.sub('\\[.*?\\]', '', text)\n",
    "        text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "        text = re.sub('<.*?>+', '', text)\n",
    "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "        text = re.sub('\\n', '', text)\n",
    "        text = re.sub('\\w*\\d\\w*', '', text)\n",
    "        return text\n",
    "    \n",
    "    def removeNumbers(self,text):\n",
    "        \"\"\" Removes integers \"\"\"\n",
    "        text = ''.join([i for i in text if not i.isdigit()])         \n",
    "        return text\n",
    "    \n",
    "    def replaceMultiExclamationMark(self,text):\n",
    "        \"\"\" Replaces repetitions of exlamation marks \"\"\"\n",
    "        text = re.sub(r\"(\\!)\\1+\", ' multiExclamation ', text)\n",
    "        return text\n",
    "\n",
    "    def replaceMultiQuestionMark(self,text):\n",
    "        \"\"\" Replaces repetitions of question marks \"\"\"\n",
    "        text = re.sub(r\"(\\?)\\1+\", ' multiQuestion ', text)\n",
    "        return text\n",
    "\n",
    "    def replaceMultiStopMark(self,text):\n",
    "        \"\"\" Replaces repetitions of stop marks \"\"\"\n",
    "        text = re.sub(r\"(\\.)\\1+\", ' multiStop ', text)\n",
    "        return text\n",
    "    \n",
    "    def replaceContraction(self,text):\n",
    "        \n",
    "        contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "                         (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\n",
    "        patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "        for (pattern, repl) in tqdm(patterns):\n",
    "            (text, count) = re.subn(pattern, repl, text)\n",
    "        return text\n",
    "    \n",
    "    def __replace(self,word, pos=None):\n",
    "        \n",
    "        \"\"\" Creates a set of all antonyms \n",
    "        for the word and if there is only \n",
    "        one antonym, it returns it \"\"\"\n",
    "        \n",
    "        antonyms = set()\n",
    "        for synset in wordnet.synsets(tqdm(word, pos=pos)):\n",
    "            for lemma in synset.lemmas():\n",
    "                for antonym in lemma.antonyms():\n",
    "                    antonyms.add(antonym.name())\n",
    "        if len(antonyms) == 1:\n",
    "            return antonyms.pop()\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def __replaceNegations(self,text):\n",
    "        \n",
    "        \"\"\" Finds \"not\" and antonym for \n",
    "        the next word and if found, replaces \n",
    "        not and the next word with the antonym \"\"\"\n",
    "        \n",
    "        i, l = 0, len(text)\n",
    "        words = []\n",
    "        while i < l:\n",
    "            word = text[i]\n",
    "            if word == 'not' and i+1 < l:\n",
    "                ant = self.__replace(text[i+1])\n",
    "                if ant:\n",
    "                    words.append(ant)\n",
    "                    i += 2\n",
    "                    continue\n",
    "            words.append(word)\n",
    "            i += 1\n",
    "        return words\n",
    "    \n",
    "    def tokenize1(self,text):\n",
    "        finalTokens = []\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = self.__replaceNegations(tokens)\n",
    "        for w in tqdm(tokens):\n",
    "            if (w not in stoplist):\n",
    "                finalTokens.append(w)\n",
    "        text = \" \".join(finalTokens)\n",
    "        return text\n",
    "    \n",
    "    def stem_words(self,text):\n",
    "        \n",
    "        stemmer = PorterStemmer()\n",
    "        return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "    \n",
    "    \n",
    "class Preprocessor_Execution():\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def Execute_Preprocessor(self,df):\n",
    "        \n",
    "        preprocess = DataFrame_Preprocessor()\n",
    "        \n",
    "        xdf, target = preprocess.Preprocessor(df)\n",
    "        \n",
    "        text = xdf.apply(lambda x: preprocess.clean_text(x))\n",
    "        \n",
    "        text = xdf.apply(lambda x: preprocess.removeNumbers(x))\n",
    "        \n",
    "        text = xdf.apply(lambda x: preprocess.replaceMultiExclamationMark(x))\n",
    "        \n",
    "        text = xdf.apply(lambda x: preprocess.replaceMultiQuestionMark(x))\n",
    "        \n",
    "        text = xdf.apply(lambda x: preprocess.replaceMultiStopMark(x))\n",
    "        \n",
    "        text = xdf.apply(lambda x: preprocess.replaceContraction(x))\n",
    "        \n",
    "        text = xdf.apply(lambda x: preprocess.tokenize1(x))\n",
    "        \n",
    "        text = xdf.apply(lambda x: preprocess.stem_words(x))\n",
    "        \n",
    "        return text,target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_test_Splitter():\n",
    "    \"\"\"\n",
    "    Split Data Into train and test set\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def Split(self,text,target):\n",
    "        \n",
    "        return train_test_split(text, target, test_size=0.2, random_state=4, stratify=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keras_Tokenizer():\n",
    "\n",
    "    \n",
    "    def __init__(self,max_features):\n",
    "        \n",
    "        self.max_features =6000\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def __label_encoding(self,y_train):\n",
    "        \"\"\"\n",
    "        Encode the given list of class labels\n",
    "        :y_train_enc: returns list of encoded classes\n",
    "        :labels: actual class labels\n",
    "        \"\"\"\n",
    "        lbl_enc = LabelEncoder()\n",
    "\n",
    "        y_train_enc = lbl_enc.fit_transform(y_train)\n",
    "        labels = lbl_enc.classes_\n",
    "\n",
    "        return y_train_enc, labels\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __word_embedding(self,train, test, max_features, max_len=200):\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            \"\"\" Keras Tokenizer class object \"\"\"\n",
    "            tokenizer = text.Tokenizer(num_words=max_features)\n",
    "            tokenizer.fit_on_texts(train)\n",
    "\n",
    "            train_data = tokenizer.texts_to_sequences(train)\n",
    "            test_data = tokenizer.texts_to_sequences(test)\n",
    "\n",
    "            \"\"\" Get the max_len \"\"\"\n",
    "            vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "            \"\"\" Padd the sequence based on the max-length \"\"\"\n",
    "            x_train = sequence.pad_sequences(train_data, maxlen=max_len, padding='post')\n",
    "            x_test = sequence.pad_sequences(test_data, maxlen=max_len, padding='post')\n",
    "            \"\"\" Return train, test and vocab size \"\"\"\n",
    "            return tokenizer, x_train, x_test, vocab_size\n",
    "        except ValueError as ve:\n",
    "            raise(ValueError(\"Error in word embedding {}\".format(ve)))\n",
    "            \n",
    "            \n",
    "    def preprocess(self,X_train, X_test):\n",
    "        \n",
    "        tokenizer,x_pad_train, x_pad_valid, vocab_size = self.__word_embedding(X_train, X_test, self.max_features)\n",
    "    \n",
    "        return tokenizer,x_pad_train, x_pad_valid, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove_Vectors_Loader():\n",
    "\n",
    "    \n",
    "    def __init__(self,FileNmae,Mode,encoding):\n",
    "        self.FileName = 'glove.6B.200d.txt'\n",
    "        self.Mode = 'r'\n",
    "        self.encoding = 'cp437'\n",
    "        \n",
    "        \n",
    "        \n",
    "    def load_Glove_Vectors(self):\n",
    "        embeddings_index={}\n",
    "        with open(self.FileName ,self.Mode ,encoding =self.encoding ) as f:\n",
    "            for line in tqdm(f):\n",
    "                values=line.split()\n",
    "                word=values[0]\n",
    "                vectors=np.asarray(values[1:],'float32')\n",
    "                embeddings_index[word]=vectors\n",
    "        f.close()\n",
    "        return embeddings_index\n",
    "    \n",
    "    def __sent2vec(self,s):\n",
    "        words = str(s).lower()\n",
    "        words = word_tokenize(words)\n",
    "        words = [w for w in words if not w in stoplist]\n",
    "        words = [w for w in words if w.isalpha()]\n",
    "        M = []\n",
    "        for w in words:\n",
    "            try:\n",
    "                M.append(embeddings_index[w])\n",
    "            except:\n",
    "                continue\n",
    "        M = np.array(M)\n",
    "        v = M.sum(axis=0)\n",
    "        if type(v) != np.ndarray:\n",
    "            return np.zeros(200)\n",
    "        return v / np.sqrt((v ** 2).sum())\n",
    "    \n",
    "    def Fit_Transform(self,X_train,X_test):\n",
    "        \n",
    "        xtrain_glove = np.array([self.__sent2vec(x) for x in tqdm(X_train)])\n",
    "        xtest_glove = np.array([self.__sent2vec(x) for x in tqdm(X_test)])\n",
    "        return xtrain_glove,xtest_glove\n",
    "    \n",
    "class Glove_Vectors_Execution():\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def Execute_Glove_Vectors(self,X_train,X_test):\n",
    "        \n",
    "\n",
    "        GL = Glove_Vectors_Loader('glove.6B.200d.txt','r','cp437')\n",
    "        \n",
    "        embeddings_index = GL.load_Glove_Vectors()\n",
    "        \n",
    "        xtrain_glove,xtest_glove  = (lambda x,y: GL.Fit_Transform(x,y))(X_train,X_test)\n",
    "        \n",
    "        return embeddings_index,xtrain_glove,xtest_glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional lstm RNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Bidirectional_lstm_Build_Pack():\n",
    "\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_length,\n",
    "                 output_length,\n",
    "                 vocab_size,\n",
    "                 optimizer,\n",
    "                 loss,\n",
    "                 metrics,\n",
    "                 batch_size,\n",
    "                 epochs,\n",
    "                 verbose):\n",
    "        \n",
    "        self.input_length =200\n",
    "        self.output_length= 200\n",
    "        self.vocab_size = 95708\n",
    "        self.optimizer = 'adam'\n",
    "        self.loss = 'binary_crossentropy'\n",
    "        self.metrics = ['acc']\n",
    "        self.batch_size = 256\n",
    "        self.epochs = 10\n",
    "        self.verbose = 1\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    def build_rnn(self,vocab_size,output_dim, input_dim):\n",
    "\n",
    "        model = Sequential([\n",
    "            keras.layers.Embedding(self.vocab_size,output_dim = self.output_length,\n",
    "                                  input_length = self.input_length),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Bidirectional(keras.layers.LSTM(256,return_sequences=True)),\n",
    "            keras.layers.GlobalMaxPool1D(),\n",
    "            keras.layers.Dense(225),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(150),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(95),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            keras.layers.Dense(64),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            keras.layers.Dense(34),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            keras.layers.Dense(32),\n",
    "            keras.layers.Dense(output_dim, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def Compile_and_Fit(self,x_pad_train,y_train,x_pad_valid,y_test,rnn_model):\n",
    "        \n",
    "        try:\n",
    "    \n",
    "            rnn_model.compile(optimizer=self.optimizer, loss=self.loss, metrics=self.metrics)\n",
    "\n",
    "\n",
    "            rnn_model.fit(x_pad_train, \n",
    "                                    y_train,\n",
    "                                    batch_size=self.batch_size,\n",
    "                                   epochs=self.epochs,\n",
    "                                   verbose= self.verbose)\n",
    "\n",
    "            score = rnn_model.evaluate(x_pad_valid, y_test, verbose=1)\n",
    "\n",
    "            print(\"Loss:%.3f Accuracy: %.3f\" % (score[0], score[1]))\n",
    "\n",
    "            return rnn_model\n",
    "        \n",
    "        except ValueError as Model_Error:\n",
    "            raise(ValueError(\"Model Compiling Error {}\".format(Model_Error)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End  Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Pipeline_Excecution Object Created\n",
      "DataFrame Shape (17880, 18)\n",
      "After Train Test Split (14304,) (3576,) (14304,) (3576,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1705it [00:00, 16926.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Tokenization <keras_preprocessing.text.Tokenizer object at 0x00000226B06CC790> (14304, 200) (3576, 200) 112655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:24, 16617.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 14304/14304 [00:20<00:00, 711.48it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3576/3576 [00:05<00:00, 709.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Glove Embedding 400000 (14304, 200) (3576, 200)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 200)          19141600  \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 200, 200)          800       \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200, 512)          935936    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 225)               115425    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 225)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150)               33900     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 95)                14345     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 95)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                6144      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 34)                2210      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                1120      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 20,251,513\n",
      "Trainable params: 20,251,113\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "56/56 [==============================] - 284s 5s/step - loss: 0.1873 - acc: 0.9506\n",
      "Epoch 2/10\n",
      "56/56 [==============================] - 329s 6s/step - loss: 0.0516 - acc: 0.9837\n",
      "Epoch 3/10\n",
      "56/56 [==============================] - 358s 6s/step - loss: 0.0134 - acc: 0.9959\n",
      "Epoch 4/10\n",
      "56/56 [==============================] - 369s 7s/step - loss: 0.0083 - acc: 0.9980\n",
      "Epoch 5/10\n",
      "56/56 [==============================] - 368s 7s/step - loss: 0.0048 - acc: 0.9991\n",
      "Epoch 6/10\n",
      "56/56 [==============================] - 369s 7s/step - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 7/10\n",
      "56/56 [==============================] - 361s 6s/step - loss: 9.3156e-04 - acc: 0.9999\n",
      "Epoch 8/10\n",
      "56/56 [==============================] - 356s 6s/step - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 9/10\n",
      "56/56 [==============================] - 352s 6s/step - loss: 2.4693e-04 - acc: 0.9999\n",
      "Epoch 10/10\n",
      "56/56 [==============================] - 351s 6s/step - loss: 0.0023 - acc: 0.9999\n",
      "112/112 [==============================] - 37s 331ms/step - loss: 0.1243 - acc: 0.9832\n",
      "Loss:0.124 Accuracy: 0.983\n"
     ]
    }
   ],
   "source": [
    "class End_Pipeline_Excecution():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        print(\"End_Pipeline_Excecution Object Created\")\n",
    "        \n",
    "        \n",
    "    def Execute_Fulll_Pipeline(self):\n",
    "        \n",
    "        load = DataFrame_Loader()\n",
    "        PE = Preprocessor_Execution()\n",
    "        TS = Train_test_Splitter()\n",
    "        KT = Keras_Tokenizer(6000)\n",
    "        GVE  = Glove_Vectors_Execution()\n",
    "        Rnn_Model = RNN_Bidirectional_lstm_Build_Pack(200,200,95708,'adam','binary_crossentropy',['acc'],256,10,1)\n",
    "        df = load.load_data_files(path1)\n",
    "        print(\"DataFrame Shape\",df.shape)\n",
    "        text,target = PE.Execute_Preprocessor(df)\n",
    "        X_train, X_test, y_train, y_test = TS.Split(text,target)\n",
    "        print(\"After Train Test Split\",X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "        tokenizer, x_pad_train, x_pad_valid, vocab_size = KT.preprocess(X_train, X_test)\n",
    "        print(\"After Tokenization\",tokenizer, x_pad_train.shape, x_pad_valid.shape, vocab_size)\n",
    "        embeddings_index,xtrain_glove,xtest_glove  = GVE.Execute_Glove_Vectors(x_pad_train,x_pad_valid)\n",
    "        print(\"After Glove Embedding\",len(embeddings_index),xtrain_glove.shape,xtest_glove.shape)\n",
    "        rnn_model = Rnn_Model.build_rnn(vocab_size,1,200)\n",
    "        print(rnn_model.summary())\n",
    "        rnn_model = Rnn_Model.Compile_and_Fit(x_pad_train,y_train,x_pad_valid,y_test,rnn_model)\n",
    "        return tokenizer,rnn_model,x_pad_valid,y_test\n",
    "    \n",
    "End = End_Pipeline_Excecution()\n",
    "tokenizer,rnn_model,x_pad_valid,y_test = End.Execute_Fulll_Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_preds Shape :: (3576, 1)\n",
      "(3576, 1)\n",
      "pred\n",
      "0       3437\n",
      "1        139\n",
      "dtype: int64\n",
      "0.9832214765100671\n",
      "[[3390   13]\n",
      " [  47  126]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      3403\n",
      "           1       0.91      0.73      0.81       173\n",
      "\n",
      "    accuracy                           0.98      3576\n",
      "   macro avg       0.95      0.86      0.90      3576\n",
      "weighted avg       0.98      0.98      0.98      3576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_preds = rnn_model.predict(x_pad_valid)\n",
    "\n",
    "print(\"y_preds Shape ::\",y_preds.shape)\n",
    "\n",
    "\n",
    "for arr in y_preds:\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i]>0.5:\n",
    "            arr[i] = 1\n",
    "        else:\n",
    "            arr[i] = 0\n",
    "\n",
    "            \n",
    "y_preds = y_preds.astype('int32')\n",
    "\n",
    "pred_df = pd.DataFrame(y_preds, columns=['pred'])\n",
    "\n",
    "print(pred_df.shape)\n",
    "pred_df.head()\n",
    "\n",
    "print(pred_df.value_counts())\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(metrics.accuracy_score(y_test, pred_df))\n",
    "        \n",
    "print(metrics.confusion_matrix(y_test, pred_df))\n",
    "        \n",
    "print(metrics.classification_report(y_test, pred_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
